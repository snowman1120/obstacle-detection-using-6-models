{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLEPgHknmiCW"
   },
   "source": [
    "# Convolutional Neural Networks for Artwork recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2qJgtj9Cf2u"
   },
   "source": [
    "## Imports and storage mounting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from typing import Tuple\n",
    "from tqdm.notebook import tqdm\n",
    "import cv2, json, skvideo.io\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, OrderedDict\n",
    "from IPython.display import display, display_markdown\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import classification_report, roc_curve, confusion_matrix\n",
    "import seaborn as sn\n",
    "from typing import Optional, Callable, Any\n",
    "from itertools import repeat\n",
    "import torch\n",
    "from torchvision.datasets import VisionDataset\n",
    "from torchvision.transforms import transforms\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, MaxPooling2D, LeakyReLU, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, Input, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import VGG19, MobileNetV2, InceptionV3\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q39oORWQjQCD"
   },
   "source": [
    "The storage used below is a Google Drive folder, where I uploaded the artwork videos and related files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Ajgc5CyLNUm",
    "outputId": "233d3872-2016-4f1e-cebe-ef6f7888c4dd"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Please enter your base dir and files dir\n",
    "\n",
    "base_dir\n",
    "    - final_result.ipynb\n",
    "    - files_dir (PEDES)\n",
    "        - Videos_Cloudy_North Side folder\n",
    "        - Videos_Cloudy_South Side folder\n",
    "        - Videos_Day_North Side folder\n",
    "        - Videos_Day_South Side folder\n",
    "        - Videos_Night_North Side folder\n",
    "        - Videos_Night_South Side folder\n",
    "        - new.csv\n",
    "        - desc.csv\n",
    "I have made new.csv from desc.csv using function InfoUpdate():\n",
    "    There are informations of non-existed video files in desc.csv.\n",
    "    So I have gotten new.csv file.\n",
    "'''\n",
    "base_dir = Path(r\"E:\\working\\obstacle_detection\")\n",
    "files_dir = base_dir / \"PEDES\"\n",
    "video_info = \"new.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Considering information for existed video files ##\n",
    "def InfoUpdate():\n",
    "    dataset_info = pd.read_csv(files_dir / \"desc.csv\")\n",
    "    new = dataset_info.copy()\n",
    "    inds = []\n",
    "    for i, row in dataset_info.iterrows():\n",
    "        if not (files_dir / row[\"path\"]).is_file():\n",
    "            inds.append(i)\n",
    "    inds.sort(reverse=True)\n",
    "    for id in inds:\n",
    "        new = new.drop(id)\n",
    "    new.to_csv(files_dir/\"new.csv\")\n",
    "InfoUpdate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kK9MpqAzorAj"
   },
   "source": [
    "## Dataset generation from artwork videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VaRvAOydjtO9"
   },
   "source": [
    "Here, \n",
    "- we generate data generator using pytorch from the artwork videos.\n",
    "- then, we get dataset from the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "First, we consider auxiliary functions.\n",
    "'''\n",
    "\n",
    "def get_video_rotation(video_path: str):\n",
    "    \"\"\" Reads video rotation from video metadata using scikit-video.\n",
    "\n",
    "    Works well with .mp4 files, but has trouble with .mov files due to\n",
    "    different metadata structure; .mov files also often do not contain rotation\n",
    "    information, but openCV seems to read frames from .mov files in the correct\n",
    "    orientation anyway, so things balance out.\n",
    "\n",
    "    :param video_path: path to the video file\n",
    "    :return: rotation of video in int degrees (e.g. 0, 90, 180)\n",
    "    \"\"\"\n",
    "    orientation = 0\n",
    "    try:\n",
    "        metadata = skvideo.io.ffprobe(video_path)\n",
    "\n",
    "        for tags in metadata[\"video\"][\"tag\"]:\n",
    "            # we get OrderedDicts here\n",
    "            if tags[\"@key\"] == \"rotate\":\n",
    "                orientation = int(tags[\"@value\"])\n",
    "\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return orientation\n",
    "\n",
    "def resize_and_rescale(img, fr_size: int, mean: float, std: float):\n",
    "    \"\"\"\n",
    "    Resizes frames to the desired shape and scale. See\n",
    "    https://stackoverflow.com/a/58096430 for scale conversion explanation.\n",
    "    \"\"\"\n",
    "    img = tf.image.resize(img, (fr_size, fr_size))\n",
    "    return (tf.cast(img, tf.float32) - mean) / std\n",
    "\n",
    "def random_random_crop(img: tf.Tensor):\n",
    "    \"\"\"\n",
    "    Randomly crops 50% of the provided frames. The cropped frames will have a\n",
    "    random size corresponding to 70-90% of their original height, and 70-90% of\n",
    "    their original width (the 2 percentages are generated independently). The\n",
    "    3rd axis of the frame tensor, i.e. the image channels, is not modified.\n",
    "\n",
    "    NOTE the use of only tf.random functions below, regular Python\n",
    "    random.random functions won't work with Tensorflow.\n",
    "\n",
    "    :param img: the frame to be cropped\n",
    "    :return: the cropped frame\n",
    "    \"\"\"\n",
    "    # lambda function that returns a random boolean, so that random cropping\n",
    "    # is only applied to 50% of the frames\n",
    "    rnd_bool = lambda: tf.random.uniform(shape=[], minval=0, maxval=2,\n",
    "                                         dtype=tf.int32) != 0\n",
    "\n",
    "    # lambda function that returns a random float in the range 0.7-0.9\n",
    "    rnd_pcnt = lambda: tf.random.uniform(shape=[], minval=0.7, maxval=0.9,\n",
    "                                         dtype=tf.float32)\n",
    "\n",
    "    h, w = int(float(tf.shape(img)[0]) * rnd_pcnt()), int(\n",
    "        float(tf.shape(img)[1]) * rnd_pcnt())\n",
    "\n",
    "    return tf.cond(rnd_bool(),\n",
    "                   lambda: tf.image.random_crop(img, size=[h, w, 3]),\n",
    "                   lambda: img)\n",
    "\n",
    "def random_modifications(img, label):\n",
    "    \"\"\"\n",
    "    Applies random modifications to the frame provided. The modifications may\n",
    "    include variation in brightness, horizontal flipping, and random cropping\n",
    "    (or none of the previous, in which case the frame will be returned\n",
    "    untouched).\n",
    "\n",
    "    :param img: the frame to be modified\n",
    "    :param label: the corresponding label of the frame, this is returned as is\n",
    "    :return: the modified frame\n",
    "    \"\"\"\n",
    "    img = random_random_crop(img)\n",
    "    img = tf.image.random_brightness(img, 0.2)\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    # img = tf.image.random_hue(img, 0.2)\n",
    "    return img, label\n",
    "\n",
    "def split_dataset(dataset: tf.data.Dataset, validation_data_fraction: float):\n",
    "    \"\"\"\n",
    "    Splits a dataset of type tf.data.Dataset into a training and validation\n",
    "    dataset using given ratio. Fractions are rounded up to two decimal places.\n",
    "    From https://stackoverflow.com/a/59696126\n",
    "\n",
    "    :param dataset: the input dataset to split\n",
    "    :param validation_data_fraction: the fraction of the validation data as a\n",
    "     float between 0 and 1+\n",
    "    :return: a tuple of two tf.data.Datasets as (training, validation)\n",
    "    \"\"\"\n",
    "    \n",
    "    validation_data_percent = round(validation_data_fraction*100)\n",
    "    if not (0 <= validation_data_percent <= 100):\n",
    "        raise ValueError(\"validation data fraction must be âˆˆ [0,100]\")\n",
    "\n",
    "    dataset = dataset.enumerate()\n",
    "    train_dataset = dataset.filter(lambda f, data: f % 100 > validation_data_percent)\n",
    "    validation_dataset = dataset.filter(lambda f, data: f % 100 <= validation_data_percent)\n",
    "\n",
    "    # remove enumeration\n",
    "    train_dataset = train_dataset.map(lambda f, data: data)\n",
    "    validation_dataset = validation_dataset.map(lambda f, data: data)\n",
    "\n",
    "    return train_dataset, validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### using pytorch  ###\n",
    "class ObstacleImageDataset(VisionDataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            *,\n",
    "            videos_dir: Path,\n",
    "            annotations_file_path: Path,\n",
    "            train: bool = True,\n",
    "            size: int = 3000,\n",
    "            transform: Optional[Callable] = None,\n",
    "            target_transform: Optional[Callable] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        if not transform is passed, PIL images will be returned\n",
    "        :param videos_dir:\n",
    "        :type videos_dir:\n",
    "        :param annotations_file_path:\n",
    "        :type annotations_file_path:\n",
    "        :param train:\n",
    "        :type train:\n",
    "        :param transform:\n",
    "        :type transform:\n",
    "        :param target_transform:\n",
    "        :type target_transform:\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            root=\"\",  # we are not making use of the root dir attribute of the class we are inheriting from\n",
    "            transform=transform,\n",
    "            target_transform=target_transform,\n",
    "        )\n",
    "\n",
    "        assert videos_dir.is_dir(), f\"{videos_dir} does not exist!\"\n",
    "        assert annotations_file_path.is_file(), f\"{annotations_file_path} not a file!\"\n",
    "\n",
    "        self.annotations: pd.DataFrame = pd.read_csv(annotations_file_path)\n",
    "        self.videos_dir: Path = videos_dir\n",
    "        self.size: int = size\n",
    "        self.transform: Optional[Callable] = transform\n",
    "        self.target_transform: Optional[Callable] = target_transform\n",
    "\n",
    "        self.obstacle_types: list[str] = sorted(\n",
    "            self.annotations[\"id\"].unique()\n",
    "        )\n",
    "        if len(self.obstacle_types) > self.size:\n",
    "            raise ValueError(\n",
    "                f\"{self.size} cannot be lower than {len(self.obstacle_types)}.\"\n",
    "            )\n",
    "\n",
    "        # calculate number of frames to extract per obstacle type\n",
    "        frames_per_obs, remaining = divmod(self.size, len(self.obstacle_types))\n",
    "        self.frames_per_obs_type: dict[str, int] = dict(\n",
    "            zip(self.obstacle_types, repeat(frames_per_obs))\n",
    "        )\n",
    "\n",
    "        if remaining != 0:\n",
    "            # distribute remaining number of frames to as many obstacle types as possible\n",
    "            # by definition, remaining < len(self.obstacle_types)\n",
    "            for i, k in enumerate(self.frames_per_obs_type.keys(), start=1):\n",
    "                self.frames_per_obs_type[k] += 1\n",
    "                if i == remaining:\n",
    "                    break\n",
    "        assert self.size == sum(self.frames_per_obs_type.values()), \\\n",
    "                f\"{self.size} != {sum(self.frames_per_obs_type.values())}\"\n",
    "\n",
    "        # check that enough frames are available in the videos for all obstacle types\n",
    "        self.available_frames_per_obs_type: dict[str, int] = \\\n",
    "                (self.annotations.groupby([\"id\"])[\"total_frames_proper\"].sum().to_dict())\n",
    "\n",
    "        obs_types_with_insufficient_frames = []\n",
    "        for obs_type, n_frames in self.frames_per_obs_type.items():\n",
    "            if self.available_frames_per_obs_type[obs_type] < n_frames:\n",
    "                obs_types_with_insufficient_frames.append(obs_type)\n",
    "        if obs_types_with_insufficient_frames:\n",
    "            raise ValueError(\n",
    "                f\"Insufficient number of frames available for obstacle type(s): {obs_types_with_insufficient_frames}.\"\n",
    "            )\n",
    "        self.frames_info = []\n",
    "        len_obstacle = len(self.obstacle_types)\n",
    "\n",
    "        for obs_type in self.obstacle_types:\n",
    "            videos_per_obs_type = self.annotations.loc[\n",
    "                self.annotations[\"id\"] == obs_type\n",
    "                ]\n",
    "            # check that all videos for obstacle type exist\n",
    "            assert all(\n",
    "                Path(self.videos_dir, vid_path)\n",
    "                for vid_path in videos_per_obs_type[\"path\"]\n",
    "            ), f\"Missing videos for {obs_type}!\"\n",
    "\n",
    "            available_frames_for_obs = self.available_frames_per_obs_type[obs_type]\n",
    "            needed_frames_for_obs = self.frames_per_obs_type[obs_type]\n",
    "            interval = available_frames_for_obs // needed_frames_for_obs\n",
    "            assert interval >= 1\n",
    "            label = torch.zeros(len_obstacle)\n",
    "            label[self.obstacle_types.index(obs_type)] = 1\n",
    "            obs_frames = []\n",
    "            for _, vid_info in videos_per_obs_type.iterrows():\n",
    "\n",
    "                frames_indices_to_extract = [\n",
    "                    [label, Path(self.videos_dir, vid_info[\"path\"]), i]\n",
    "                    for i in range(\n",
    "                        vid_info[\"proper_start_frame\"],\n",
    "                        vid_info[\"proper_end_frame\"] - 1,\n",
    "                        interval,\n",
    "                    )\n",
    "                ]\n",
    "                obs_frames.extend(frames_indices_to_extract)\n",
    "\n",
    "            assert len(obs_frames) >= needed_frames_for_obs\n",
    "            obs_frames = obs_frames[:needed_frames_for_obs]\n",
    "\n",
    "            self.frames_info.extend(obs_frames)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        label, video_path, frame_index = self.frames_info[index]\n",
    "\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        cap.set(1, frame_index)\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            raise ValueError\n",
    "        # read video orientation once, in case of error value will be 0\n",
    "        orientation = get_video_rotation(str(video_path))\n",
    "        # rot90() below rotates counter-clockwise, so by providing a\n",
    "        # negative k the frames are rotated clockwise\n",
    "        k = orientation // -90\n",
    "        # frame = tf.image.rot90(frame, k)\n",
    "        frame = np.rot90(frame,k)\n",
    "        # openCv reads frames in BGR format, convert to RGB\n",
    "        # frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = frame[:, :, ::-1]\n",
    "        frame = torch.from_numpy(frame.copy())\n",
    "        frame = frame.type(torch.FloatTensor)\n",
    "\n",
    "        return (frame, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dataset_from_videos(files_dir: Path, dataset_csv_info_file: str,\n",
    "                        total_frames: int = 750, batch_size: int = 128,\n",
    "                        img_normalization_params: Tuple[float, float] = (\n",
    "                                0.0, 255.0), frame_size: int = 224,\n",
    "                        train_val_test_percentages: Tuple[int, int, int] = (\n",
    "                                70, 30, 0)):\n",
    "    \"\"\"\n",
    "    Generates train, validation and test tf.data.Datasets from the provided\n",
    "    video files.\n",
    "\n",
    "    :param files_dir: path of the directory containing the videos\n",
    "    :param dataset_csv_info_file: name of csv file containing information about\n",
    "     the videos, must be located in files_dir\n",
    "    :param img_normalization_params: tuple of doubles (mean, standard_deviation)\n",
    "     to use for normalizing the extracted frames, e.g. if (0.0, 255.0) is\n",
    "     provided, the frames are normalized to the range [0, 1], see this comment\n",
    "     for explanation of how to convert between the two\n",
    "     https://stackoverflow.com/a/58096430\n",
    "    :param total_frames: total number of frames to extract for all artwork\n",
    "    :param frame_size: the size of the final resized square frames, this is\n",
    "     dictated by the needs of the underlying NN that will be used\n",
    "    :param train_val_test_percentages: tuple specifying how to split the\n",
    "     generated dataset into train, validation and test datasets, the provided\n",
    "     ints must add up to 100\n",
    "    :param batch_size: batch size for datasets\n",
    "    :return: a tuple of train, validation and test tf.data.Datasets, as well\n",
    "     as a list of the artworks ids\n",
    "    \"\"\"\n",
    "    assert sum(train_val_test_percentages) == 100, \"Percentages must add up to 100!\"\n",
    "    dataset_info = pd.read_csv(files_dir / dataset_csv_info_file)\n",
    "\n",
    "    # make sure all files in csv are present\n",
    "    for _, row in dataset_info.iterrows():\n",
    "        assert (files_dir / row[\"path\"]).is_file(), files_dir / row[\"path\"]\n",
    "\n",
    "    # sort artwork ids in alphabetical order, this is important as it\n",
    "    # determines how the CNN outputs its predictions\n",
    "    artwork_dict = {artwork_id: i for i, artwork_id in enumerate(sorted(dataset_info[\"id\"].unique()))}\n",
    "    artwork_list = list(artwork_dict.keys())\n",
    "\n",
    "    # create dataset, output_shapes are set to (None, None, 3), since the\n",
    "    # extracted frames are not initially resized,\n",
    "    # to allow applying variations to the train dataset only below\n",
    "\n",
    "    dt = tf.data.Dataset.from_generator(\n",
    "    lambda: ObstacleImageDataset(videos_dir=files_dir, \\\n",
    "        annotations_file_path=Path(files_dir, dataset_csv_info_file), size=total_frames, \\\n",
    "            transform=transforms.Compose([transforms.ToTensor(), transforms.Resize(size=1080 // 3)]),), \\\n",
    "                output_types=(tf.float32, tf.float32), output_shapes=((None, None, 3), (27)))\n",
    "\n",
    "    # TODO calculate the datasets' sizes, perhaps print them, and also use them\n",
    "    #  in the shuffling of the train dt below can be calculated like so:\n",
    "    #  (num of classes * total_frames) * % of dataset\n",
    "\n",
    "    # split into train, validation & test datasets\n",
    "    train, val, test = train_val_test_percentages\n",
    "    print(train_val_test_percentages)\n",
    "    train_dataset, validation_and_test = split_dataset(dt, (val + test)/100)\n",
    "    validation_dataset, test_dataset = split_dataset(validation_and_test,\n",
    "                                                     test / (val + test))\n",
    "\n",
    "    mean, std = img_normalization_params\n",
    "\n",
    "    # apply necessary conversions (normalization, random modifications,\n",
    "    # batching & caching) to the created datasets\n",
    "    # see https://www.tensorflow.org/datasets/keras_example for batching and\n",
    "    # caching explanation\n",
    "    AUTO = tf.data.experimental.AUTOTUNE  # auto-optimise dataset mapping below\n",
    "    \n",
    "    # print(list(train_dataset))\n",
    "    train_dataset = train_dataset \\\n",
    "        .map(random_modifications, num_parallel_calls=AUTO) \\\n",
    "        .map(lambda x, y: (\n",
    "        resize_and_rescale(x, fr_size=frame_size, mean=mean, std=std), y),\n",
    "             num_parallel_calls=AUTO) \\\n",
    "        .cache() \\\n",
    "        .shuffle(1000) \\\n",
    "        .batch(batch_size) \\\n",
    "        .prefetch(AUTO)\n",
    "\n",
    "    validation_dataset = validation_dataset \\\n",
    "        .map(lambda x, y: (\n",
    "        resize_and_rescale(x, fr_size=frame_size, mean=mean, std=std), y),\n",
    "             num_parallel_calls=AUTO) \\\n",
    "        .batch(batch_size) \\\n",
    "        .cache() \\\n",
    "        .prefetch(AUTO)\n",
    "\n",
    "    test_dataset = test_dataset \\\n",
    "        .map(lambda x, y: (\n",
    "        resize_and_rescale(x, fr_size=frame_size, mean=mean, std=std), y),\n",
    "             num_parallel_calls=AUTO) \\\n",
    "        .batch(batch_size) \\\n",
    "        .cache() \\\n",
    "        .prefetch(AUTO)\n",
    "\n",
    "    return train_dataset, validation_dataset, test_dataset, artwork_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train function ##\n",
    "def train_evaluate_save(model, model_name: str, files_dir: Path, dataset_csv_info_file: str, total_frames: int = 29*300,\n",
    "                        img_normalization_params: Tuple[float, float] = (0.0, 255.0), frame_size: int = 224,\n",
    "                        batch_size: int = 32, train_val_test_percentages: Tuple[int, int, int] = (70, 20, 10),\n",
    "                        epochs=20):\n",
    "    \"\"\"\n",
    "    Consolidates model training and evaluation, as well as presentation of the results. Additionally, the trained\n",
    "    model is saved to disk, both in its original form, as well as converted to the Tensorflow Lite format. All\n",
    "    relevant information about the model (evaluation results, plots, other stats) are save to disk as well.\n",
    "\n",
    "    :param model: the model to be trained\n",
    "    :param model_name: the preferred name for the model, used for naming the folder where the training results are saved\n",
    "    :param files_dir: path of the directory containing the videos\n",
    "    :param dataset_csv_info_file: name of csv file containing information about the videos, must be located in files_dir\n",
    "    :param total_frames: total number of frames to extract for all artwork\n",
    "    :param img_normalization_params: tuple of doubles (mean, standard_deviation) to use for normalizing the extracted\n",
    "     frames, e.g. if (0.0, 255.0) is provided, the frames are normalized to the range [0, 1], see this comment for\n",
    "     explanation of how to convert between the two https://stackoverflow.com/a/58096430\n",
    "    :param frame_size: the size of the final resized square frames, this is dictated by the needs of the underlying NN\n",
    "     that will be used in the training\n",
    "    :param batch_size: batch size for datasets\n",
    "    :param train_val_test_percentages: tuple specifying how to split the generated dataset into train, validation and\n",
    "     test datasets, the provided ints must add up to 100\n",
    "    :param epochs: the number of epochs to train the model\n",
    "    \"\"\"\n",
    "    pd.options.display.float_format = '{:,.3f}'.format\n",
    "    # folder to save info about model\n",
    "    info_dir = base_dir / model_name / \"model_info\"\n",
    "    info_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(\"Generating/splitting dataset...\", \"\\n\", flush=True)\n",
    "    train_dt, val_dt, test_dt, artwork_list = dataset_from_videos(files_dir=files_dir, total_frames=total_frames,\n",
    "                                                                  dataset_csv_info_file=dataset_csv_info_file,\n",
    "                                                                  batch_size=batch_size, frame_size=frame_size,\n",
    "                                                                  train_val_test_percentages=train_val_test_percentages,\n",
    "                                                                  img_normalization_params=img_normalization_params)\n",
    "\n",
    "    print(\"Creating model...\", \"\\n\", flush=True)\n",
    "    model = model(len(artwork_list))\n",
    "\n",
    "    # saves model train history logs, which can be visualised with TensorBoard\n",
    "    log_dir = base_dir / \"logs\" / \"fit\" / f\"{model_name}_{datetime.now().strftime('%Y%m%d%H%M')}\"\n",
    "    tb_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    # train model\n",
    "    print(\"Training model...\", \"\\n\", flush=True)\n",
    "    model_train_info = model.fit(train_dt, epochs=epochs, validation_data=val_dt, callbacks=[tb_callback])\n",
    "    print(\"Finished training!\", \"\\n\", flush=True)\n",
    "\n",
    "    # save trained model to disk, also convert to Tensorflow Lite format\n",
    "    save_model(model, model_name, artwork_list)\n",
    "\n",
    "    # evaluation\n",
    "    evaluation = model.evaluate(test_dt)\n",
    "    eval_res = pd.DataFrame.from_dict({k: [v] for k, v in zip([\"Test loss\", \"Test accuracy\"], evaluation)})\n",
    "    eval_res.to_csv(info_dir / \"evaluation_results.csv\")\n",
    "    display_markdown(\"#### Evaluation results\", raw=True)\n",
    "    display(eval_res)\n",
    "\n",
    "    # model predictions\n",
    "    predicted_labels = model.predict(test_dt)\n",
    "    predicted_labels = np.argmax(predicted_labels, axis=1)\n",
    "\n",
    "    actual_labels = np.concatenate([label for _, label in test_dt], axis=0)\n",
    "    actual_labels = np.argmax(actual_labels, axis=1)  # labels are in categorical form (one_hot), convert them back\n",
    "\n",
    "    # classification report\n",
    "    report = classification_report(actual_labels, predicted_labels, target_names=artwork_list, output_dict=True)\n",
    "    report = pd.DataFrame(report)\n",
    "    report.to_csv(info_dir / \"classification_report.csv\")\n",
    "    display_markdown(\"### Classification report\", raw=True)\n",
    "    display(report)\n",
    "\n",
    "    # plots\n",
    "    display_markdown(\"### Training history plots & confusion matrix\", raw=True)\n",
    "    ep = np.array(model_train_info.epoch) + 1\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(5, 15))\n",
    "\n",
    "    # training and validation accuracy plot\n",
    "    axes[0].plot(ep, model_train_info.history['accuracy'], \"bo\", label='Training accuracy')\n",
    "    axes[0].plot(ep, model_train_info.history['val_accuracy'], \"b\", label='Validation accuracy')\n",
    "    axes[0].set_xlabel(\"Epochs\")\n",
    "    axes[0].set_ylabel(\"Training and validation accuracy\")\n",
    "    axes[0].set_title(\"Training and validation accuracy\")\n",
    "    axes[0].legend()\n",
    "    axes[0].tick_params(axis='both', which='major')\n",
    "\n",
    "    # training and validation loss plot\n",
    "    axes[1].plot(ep, model_train_info.history['loss'], \"bo\", label='Training loss', color=\"red\")\n",
    "    axes[1].plot(ep, model_train_info.history['val_loss'], \"b\", label='Validation loss', color=\"red\")\n",
    "    axes[1].set_xlabel(\"Epochs\")\n",
    "    axes[1].set_ylabel(\"Training and validation loss\")\n",
    "    axes[1].set_title(\"Training and validation loss\")\n",
    "    axes[1].legend()\n",
    "    axes[1].tick_params(axis='both', which='major')\n",
    "\n",
    "    # confusion matrix\n",
    "    cm = confusion_matrix(actual_labels, predicted_labels)\n",
    "    df_cm = pd.DataFrame(cm, artwork_list, artwork_list)\n",
    "    df_cm.to_csv(info_dir / \"confusion_matrix.csv\")\n",
    "    sn.set(font_scale=.7)\n",
    "    sn.heatmap(df_cm, ax=axes[2], vmin=0, annot=True, cmap=\"YlGnBu\", fmt=\"d\", linewidths=0.1, linecolor=\"black\")\n",
    "\n",
    "    # display and save plots to files\n",
    "    fig.savefig(info_dir / \"graphs.svg\", bbox_inches=\"tight\")\n",
    "    fig.savefig(info_dir / \"graphs.pdf\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    # save training history\n",
    "    with open(info_dir / \"train_history.json\", \"w+\") as f:\n",
    "        json.dump(model_train_info.history, f, indent=4)\n",
    "\n",
    "    # save a few other details about the model\n",
    "    other_info = {\n",
    "        \"batch_size\": batch_size,\n",
    "        \"epochs\": epochs,\n",
    "        \"frame_size\": frame_size,\n",
    "        \"img_normalization_params\": img_normalization_params,\n",
    "        \"model_name\": model_name,\n",
    "        \"train_val_test_percentages\": train_val_test_percentages\n",
    "    }\n",
    "    with open(info_dir / \"other_info.json\", \"w+\") as f:\n",
    "        json.dump(other_info, f, indent=4)\n",
    "\n",
    "def save_model(trained_model, model_name: str, artwork_list: list):\n",
    "    \"\"\"\n",
    "    Saves the provided model to disk, and also converts it to the Tensorflow Lite format.\n",
    "\n",
    "    :param trained_model: the trained model\n",
    "    :param model_name: name to be used when saving the model\n",
    "    :param artwork_list: list containing the artwork ids sorted according to the model outputs\n",
    "    \"\"\"\n",
    "    # save model\n",
    "    print(\"Saving model to file...\", flush=True)\n",
    "    saved_model_path = base_dir / model_name / \"saved_model\"\n",
    "    trained_model.save(saved_model_path)\n",
    "\n",
    "    # Convert model to tflite\n",
    "    # first convert to normal tflite\n",
    "    converter = tf.lite.TFLiteConverter.from_saved_model(str(saved_model_path))\n",
    "    tflite_model = converter.convert()\n",
    "\n",
    "    # second convert to quantized tflite\n",
    "    print(\"Converting to tflite format...\", flush=True)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    tflite_quant_model = converter.convert()\n",
    "\n",
    "    tflite_dir = base_dir / model_name / \"tflite\"\n",
    "    tflite_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with tf.io.gfile.GFile(str(tflite_dir / f\"{model_name}.tflite\"), \"wb\") as f:\n",
    "        f.write(tflite_model)\n",
    "\n",
    "    with tf.io.gfile.GFile(str(tflite_dir / f\"{model_name}_quant.tflite\"), \"wb\") as f:\n",
    "        f.write(tflite_quant_model)\n",
    "\n",
    "    # save txt file with list of labels, ready for use in mobile device\n",
    "    labels_file = tflite_dir / f\"{model_name}_labels.txt\"\n",
    "    with open(labels_file, \"w+\") as f:\n",
    "        for label in list(artwork_list):\n",
    "            f.write(label + \"\\n\")\n",
    "    print(\"Done!\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2WCbEAXOrKVt"
   },
   "source": [
    "## CNN training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KOIGcmS-RI18"
   },
   "source": [
    "This section contains functions related to CNN training, conversion to Tensorflow Lite format, and saving the trained models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "etB9KibMshVm"
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rxicu0ttH6lj"
   },
   "source": [
    "This sections presents the results of the trained CNNs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SqueezeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below shows the configuration used for SqueezeNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Add, Activation, Concatenate, Conv2D, Dropout \n",
    "from keras.layers import Flatten, Input, GlobalAveragePooling2D, MaxPooling2D\n",
    "import keras.backend as K\n",
    "\n",
    "__version__ = '0.0.1'\n",
    "\n",
    "\n",
    "def SqueezeNet(input_shape, nb_classes, use_bypass=False, dropout_rate=None, compression=1.0):\n",
    "    \"\"\"\n",
    "    Creating a SqueezeNet of version 1.0\n",
    "    \n",
    "    Arguments:\n",
    "        input_shape  : shape of the input images e.g. (224,224,3)\n",
    "        nb_classes   : number of classes\n",
    "        use_bypass   : if true, bypass connections will be created at fire module 3, 5, 7, and 9 (default: False)\n",
    "        dropout_rate : defines the dropout rate that is accomplished after last fire module (default: None)\n",
    "        compression  : reduce the number of feature-maps (default: 1.0)\n",
    "        \n",
    "    Returns:\n",
    "        Model        : Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    input_img = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(int(96*compression), (7,7), activation='relu', strides=(2,2), padding='same', name='conv1')(input_img)\n",
    "\n",
    "    x = MaxPooling2D(pool_size=(3,3), strides=(2,2), name='maxpool1')(x)\n",
    "    \n",
    "    x = create_fire_module(x, int(16*compression), name='fire2')\n",
    "    x = create_fire_module(x, int(16*compression), name='fire3', use_bypass=use_bypass)\n",
    "    x = create_fire_module(x, int(32*compression), name='fire4')\n",
    "    \n",
    "    x = MaxPooling2D(pool_size=(3,3), strides=(2,2), name='maxpool4')(x)\n",
    "    \n",
    "    x = create_fire_module(x, int(32*compression), name='fire5', use_bypass=use_bypass)\n",
    "    x = create_fire_module(x, int(48*compression), name='fire6')\n",
    "    x = create_fire_module(x, int(48*compression), name='fire7', use_bypass=use_bypass)\n",
    "    x = create_fire_module(x, int(64*compression), name='fire8')\n",
    "    \n",
    "    x = MaxPooling2D(pool_size=(3,3), strides=(2,2), name='maxpool8')(x)\n",
    "    \n",
    "    x = create_fire_module(x, int(64*compression), name='fire9', use_bypass=use_bypass)\n",
    "\n",
    "    if dropout_rate:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "        \n",
    "    x = output(x, nb_classes)\n",
    "    model = Model(inputs=input_img, outputs=x)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\",\n",
    "                    metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def SqueezeNet_11(input_shape, nb_classes, dropout_rate=None, compression=1.0):\n",
    "    \"\"\"\n",
    "    Creating a SqueezeNet of version 1.1\n",
    "    \n",
    "    2.4x less computation over SqueezeNet 1.0 implemented above.\n",
    "    \n",
    "    Arguments:\n",
    "        input_shape  : shape of the input images e.g. (224,224,3)\n",
    "        nb_classes   : number of classes\n",
    "        dropout_rate : defines the dropout rate that is accomplished after last fire module (default: None)\n",
    "        compression  : reduce the number of feature-maps\n",
    "        \n",
    "    Returns:\n",
    "        Model        : Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    input_img = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(int(64*compression), (3,3), activation='relu', strides=(2,2), padding='same', name='conv1')(input_img)\n",
    "\n",
    "    x = MaxPooling2D(pool_size=(3,3), strides=(2,2), name='maxpool1')(x)\n",
    "    \n",
    "    x = create_fire_module(x, int(16*compression), name='fire2')\n",
    "    x = create_fire_module(x, int(16*compression), name='fire3')\n",
    "    \n",
    "    x = MaxPooling2D(pool_size=(3,3), strides=(2,2), name='maxpool3')(x)\n",
    "    \n",
    "    x = create_fire_module(x, int(32*compression), name='fire4')\n",
    "    x = create_fire_module(x, int(32*compression), name='fire5')\n",
    "    \n",
    "    x = MaxPooling2D(pool_size=(3,3), strides=(2,2), name='maxpool5')(x)\n",
    "    \n",
    "    x = create_fire_module(x, int(48*compression), name='fire6')\n",
    "    x = create_fire_module(x, int(48*compression), name='fire7')\n",
    "    x = create_fire_module(x, int(64*compression), name='fire8')\n",
    "    x = create_fire_module(x, int(64*compression), name='fire9')\n",
    "\n",
    "    if dropout_rate:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # Creating last conv10\n",
    "    x = output(x, nb_classes)\n",
    "    return Model(inputs=input_img, outputs=x)\n",
    "\n",
    "\n",
    "def output(x, nb_classes):\n",
    "    x = Conv2D(nb_classes, (1,1), strides=(1,1), padding='valid', name='conv10')(x)\n",
    "    x = GlobalAveragePooling2D(name='avgpool10')(x)\n",
    "    x = Activation(\"softmax\", name='softmax')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def create_fire_module(x, nb_squeeze_filter, name, use_bypass=False):\n",
    "    \"\"\"\n",
    "    Creates a fire module\n",
    "    \n",
    "    Arguments:\n",
    "        x                 : input\n",
    "        nb_squeeze_filter : number of filters of squeeze. The filtersize of expand is 4 times of squeeze\n",
    "        use_bypass        : if True then a bypass will be added\n",
    "        name              : name of module e.g. fire123\n",
    "    \n",
    "    Returns:\n",
    "        x                 : returns a fire module\n",
    "    \"\"\"\n",
    "    \n",
    "    nb_expand_filter = 4 * nb_squeeze_filter\n",
    "    squeeze    = Conv2D(nb_squeeze_filter,(1,1), activation='relu', padding='same', name='%s_squeeze'%name)(x)\n",
    "    expand_1x1 = Conv2D(nb_expand_filter, (1,1), activation='relu', padding='same', name='%s_expand_1x1'%name)(squeeze)\n",
    "    expand_3x3 = Conv2D(nb_expand_filter, (3,3), activation='relu', padding='same', name='%s_expand_3x3'%name)(squeeze)\n",
    "    \n",
    "    axis = get_axis()\n",
    "    x_ret = Concatenate(axis=axis, name='%s_concatenate'%name)([expand_1x1, expand_3x3])\n",
    "    \n",
    "    if use_bypass:\n",
    "        x_ret = Add(name='%s_concatenate_bypass'%name)([x_ret, x])\n",
    "        \n",
    "    return x_ret\n",
    "\n",
    "\n",
    "def get_axis():\n",
    "    axis = -1 if K.image_data_format() == 'channels_last' else 1\n",
    "    return axis\n",
    "squeezenet = SqueezeNet(input_shape=(224,224,3), nb_classes=27)\n",
    "squeezenet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_evaluate_save(model=squeezenet, model_name=\"SqueezeNetNoArt300Frames\", files_dir=files_dir, \n",
    "                    dataset_csv_info_file=video_info, total_frames=29*300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below shows the configuration used for EfficientNet0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.efficientnet import EfficientNetB0\n",
    "\n",
    "def efficientnetb0(num_classes: int):\n",
    "    eff = EfficientNetB0(include_top=False, weights=\"imagenet\", input_tensor=Input(shape=(224, 224, 3)))\n",
    "    outputs = eff.output\n",
    "    outputs = Flatten(name=\"flatten\")(outputs)\n",
    "    outputs = Dropout(0.5)(outputs)\n",
    "    outputs = Dense(num_classes, activation=\"softmax\")(outputs)\n",
    "\n",
    "    model = Model(inputs=eff.input, outputs=outputs)\n",
    "\n",
    "    for layer in eff.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\",\n",
    "                    metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "efficientnetb0(num_classes=17).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_evaluate_save(model=efficientnetb0, model_name=\"EfficientNetB0NoArt500Frames\", files_dir=files_dir, \n",
    "                    dataset_csv_info_file=video_info, total_frames=29*300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below shows the configuration used for ResNedt50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "def resnet50(num_classes: int):\n",
    "    res = ResNet50(include_top=False, weights=\"imagenet\", input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "    outputs = res.output\n",
    "    outputs = Flatten(name=\"flatten\")(outputs)\n",
    "    outputs = Dropout(0.5)(outputs)\n",
    "    outputs = Dense(num_classes, activation=\"softmax\")(outputs)\n",
    "\n",
    "    model = Model(inputs=res.input, outputs=outputs)\n",
    "\n",
    "    for layer in res.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\",\n",
    "                    metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "resnet50(num_classes=17).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_evaluate_save(model=resnet50, model_name=\"ResNet50NoArt500Frames\", files_dir=files_dir, \n",
    "                    dataset_csv_info_file=video_info, total_frames=29*300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQD4csUIsqcg"
   },
   "source": [
    "### VGG19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "spziEE2nIQO1"
   },
   "source": [
    "The cell below shows the configuration used for VGG19.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AZtgFoGxrDM9",
    "outputId": "f1523a3b-d66d-41ac-ca78-594bbcca1781"
   },
   "outputs": [],
   "source": [
    "def vgg19(num_classes: int):\n",
    "    vgg = VGG19(include_top=False, weights=\"imagenet\", \n",
    "                        input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "    outputs = vgg.output\n",
    "    outputs = Flatten(name=\"flatten\")(outputs)\n",
    "    outputs = Dropout(0.5)(outputs)\n",
    "    outputs = Dense(num_classes, activation=\"softmax\")(outputs)\n",
    "\n",
    "    model = Model(inputs=vgg.input, outputs=outputs)\n",
    "\n",
    "    for layer in vgg.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\",\n",
    "                    metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "vgg19(num_classes=17).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGQg2X_6IX8l"
   },
   "source": [
    "The training for the VGG19 based CNN was done using a dataset that contained 500 frames for each artwork, as well as the same number for the \"No artwork\" category.\n",
    "\n",
    "The training results are shown in the output of the cell below - scroll within the output to see training history graphs and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "As7YPvMKCaq2",
    "outputId": "b8a3ddc5-d0b6-4978-e6ec-65d6f339fa32"
   },
   "outputs": [],
   "source": [
    "train_evaluate_save(model=vgg19, model_name=\"VGG19NoArt500Frames\", files_dir=files_dir, \n",
    "                    dataset_csv_info_file=video_info, total_frames=29*300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mA7zMlO5spxV"
   },
   "source": [
    "### MobileNetV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZzL_wONBaNZG"
   },
   "source": [
    "The cell below shows the configuration used for MobileNet v2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZVsAVpmUaqJL"
   },
   "outputs": [],
   "source": [
    "def mobileNetV2(num_classes: int):\n",
    "    # https://www.kaggle.com/devang/transfer-learning-with-keras-and-efficientnets?scriptVersionId=24113974\n",
    "    # https://www.tensorflow.org/hub/tutorials/tf2_image_retraining\n",
    "    mnv2 = MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights=\"imagenet\",\n",
    "                       input_tensor=Input(shape=(224, 224, 3)))\n",
    "    outputs = mnv2.output\n",
    "    outputs = GlobalAveragePooling2D()(outputs)\n",
    "    outputs = BatchNormalization()(outputs)\n",
    "\n",
    "    outputs = Dense(1280, activation='relu', kernel_initializer=\"glorot_uniform\", bias_initializer='zeros')(outputs)\n",
    "    outputs = BatchNormalization()(outputs)\n",
    "\n",
    "    # final layer\n",
    "    outputs = Dense(num_classes, activation='softmax', kernel_initializer='random_uniform', bias_initializer='zeros')(outputs)\n",
    "\n",
    "    model = Model(inputs=mnv2.input, outputs=outputs)\n",
    "\n",
    "    for layer in mnv2.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\", \n",
    "                  optimizer=Adam(lr=0.0001), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "mobileNetV2(num_classes=27).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtHdMBp2awJB"
   },
   "source": [
    "The training for the MobileNet based CNN was done using a dataset that contained 500 frames for each artwork, as well as the same number for the \"No artwork\" category.\n",
    "\n",
    "The training results are shown in the output of the cell below - scroll within the output to see training history graphs and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X6oYTd1HwbR4"
   },
   "outputs": [],
   "source": [
    "train_evaluate_save(model=mobileNetV2, model_name=\"MobNetNoArt500Frames_4\", files_dir=files_dir, \n",
    "                    dataset_csv_info_file=video_info, batch_size = 4, epochs=2,\n",
    "                    total_frames=29*300, img_normalization_params=(127.5,127.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmBmWm-3eTR1"
   },
   "source": [
    "#### Older training attempt with MobileNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JlblMq7RdTVr"
   },
   "source": [
    "The cell below shows an earlier attempt to train a MobileNet CNN, with a different configuration for the final layers. The training results seem OK, but the CNN is much less accurate when used in the app. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4UKQ5CLYHCTg"
   },
   "outputs": [],
   "source": [
    "# run with older settings for mobileNetV2\n",
    "def mobileNetV2_3(num_classes: int):\n",
    "    mnv2 = MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights=\"imagenet\",\n",
    "                       input_tensor=Input(shape=(224, 224, 3)))\n",
    "    outputs = mnv2.output\n",
    "    outputs = Flatten(name=\"flatten\")(outputs)\n",
    "    outputs = Dropout(0.5)(outputs)\n",
    "    outputs = Dense(num_classes, activation=\"softmax\")(outputs)\n",
    "\n",
    "    model = Model(inputs=mnv2.input, outputs=outputs)\n",
    "\n",
    "    for layer in mnv2.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\",\n",
    "                    metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "train_evaluate_save(model=mobileNetV2_3, model_name=\"MobNetNoArt500Frames_3\", files_dir=files_dir, dataset_csv_info_file=video_info,\n",
    "                    total_frames=29*300, img_normalization_params=(127.5,127.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xIdedseF0hfW"
   },
   "source": [
    "### InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3vkmK8Fm0uhD"
   },
   "outputs": [],
   "source": [
    "def inceptionV3(num_classes: int):\n",
    "    # based on example here https://keras.io/api/applications/\n",
    "    # create the base pre-trained model\n",
    "    base_model = InceptionV3(input_shape=(224, 224, 3), weights='imagenet', include_top=False)\n",
    "\n",
    "    # add a global spatial average pooling layer\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    # add a fully-connected layer\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    # and a logistic layer\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    # this is the model we will train\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    # first: train only the top layers (which were randomly initialized)\n",
    "    # i.e. freeze all convolutional InceptionV3 layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # compile the model (should be done *after* setting layers to non-trainable)\n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# input images must be normalised to range [-1, 1], see https://www.tensorflow.org/api_docs/python/tf/keras/applications/inception_v3/preprocess_input\n",
    "train_evaluate_save(model=inceptionV3, model_name=\"InceptionV3NoArt500Frames\", files_dir=files_dir, dataset_csv_info_file=video_info,\n",
    "                    total_frames=29*300, img_normalization_params=(127.5,127.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3F7n8WXfnIvH"
   },
   "outputs": [],
   "source": [
    "def inceptionV3(num_classes: int):\n",
    "    # based on example here https://keras.io/api/applications/\n",
    "    # create the base pre-trained model\n",
    "    base_model = InceptionV3(input_shape=(224, 224, 3), weights='imagenet', include_top=False)\n",
    "\n",
    "    # add a global spatial average pooling layer\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    # add a fully-connected layer\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    # and a logistic layer\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    # this is the model we will train\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    # first: train only the top layers (which were randomly initialized)\n",
    "    # i.e. freeze all convolutional InceptionV3 layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # compile the model (should be done *after* setting layers to non-trainable)\n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "mod= inceptionV3(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYTRIiovie2l"
   },
   "source": [
    "### 0, 1, or multiple artworks (VGG19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3yu32tfkyQ9"
   },
   "source": [
    "Here I trained a CNN to distinguish between cases where there are zero, one, or multiple artworks in a frame, that could potentially be used first in the app to make sure there is exactly one artwork in the frame before trying to recognise it with one of the CNNs trained above.\n",
    "\n",
    "The goal was to train the CNN to classify a frame with one of the labels: `no_artwork`, `one_artwork`, or `multiple_artworks`. To create the dataset used for training I used the following sources for each label:\n",
    "\n",
    "*   `no_artwork`: Extracted frames from the videos that show ceiling, floor, walls, etc.\n",
    "*   `one_artwork`: Extracted frames from all videos showing a single artwork (the same that were used above to train for classify individual artworks).\n",
    "*   `multiple_artworks`: Here I used the 2 videos we have of the entire room, which I edited to contain only the parts that show at least 2 artworks. \n",
    "\n",
    "I used 1500 frames for each label, and used VGG19 as the basis of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tfCKAmC1i_jR"
   },
   "outputs": [],
   "source": [
    "train_evaluate_save(model=vgg19, model_name=\"VGG19ZeroOneMultiple1500Frames\", files_dir=files_dir, \n",
    "                    dataset_csv_info_file=\"description_export_01plus.csv\", total_frames=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "########################################\n",
    "########################################\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "9c0f54ef1f768afee8f65b12ccdfc720dd3df3ac6f5edda61e9daebfd68e3ea9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
